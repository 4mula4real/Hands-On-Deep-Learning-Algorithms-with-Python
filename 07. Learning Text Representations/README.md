
# [Chapter 7. Learning Text Representations](#)

* 7.1. Understanding Word2vec Model
* 7.2. Continuous Bag of words
* 7.3. Math of CBOW
	* 7.3.1. Deriving Forward Propagation
	* 7.3.2. Deriving Backward Propagation
* 7.4. Skip- Gram model
* 7.5. Math of Skip-Gram 
	* 7.5.1. Deriving Forward Propagation
	* 7.5.2. Deriving Backward Propagation
* 7.6. various training strategies
	* 7.6.1. Hierarchical Softmax
	* 7.6.2. Negative sampling
	* 7.6.3. Subsampling frequent words
*  [ 7.7. Building word2vec model using Gensim](#)
*  [7.8. Visualizing word embeddings in TensorBoard](#)
* 7.9. Converting documents to vectors using doc2vec
	* 7.9.1. PV-DM
	* 7.9.2. PV-DBOW
*  [7.10. Finding similar documents using Doc2vec](#)
* 7.11. Understanding skip thoughts algorithm
* 7.12 Quick thoughts for sentence embeddings